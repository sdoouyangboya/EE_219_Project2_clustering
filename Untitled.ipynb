{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "experienced-cowboy",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/boyaouyang/scikit_learn_data/glove.6B/glove.6B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b395c84cd739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0mq9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-b395c84cd739>\u001b[0m in \u001b[0;36mq9\u001b[0;34m(dimension_of_glove)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mX_test_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0membeddings_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glove_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimension_of_glove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdimension_of_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mX_train_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glove_article_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_counts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdimension_of_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mX_test_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glove_article_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_counts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdimension_of_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-b395c84cd739>\u001b[0m in \u001b[0;36mget_glove_embeddings\u001b[0;34m(dimension_of_glove)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_glove_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimension_of_glove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0membeddings_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{get_data_home()}/glove.6B/glove.6B.{dimension_of_glove}d.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/boyaouyang/scikit_learn_data/glove.6B/glove.6B.300d.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk import pos_tag, wordnet\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, roc_curve, auc\n",
    "from sklearn.svm import LinearSVC\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import get_data_home\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q1\n",
    "def cat_hist():\n",
    "\tdata = fetch_20newsgroups()\n",
    "\tcat_nfiles = [len(fetch_20newsgroups(subset='train', categories=[cat]).filenames) for cat in data.target_names]\n",
    "\tn,bins,patches = plt.hist(cat_nfiles,bins=50)\n",
    "\tplt.xlabel(\"Number of Samples in Category\")\n",
    "\tplt.ylabel(\"Number of Categories\")\n",
    "\tplt.title(\"Number of Samples in the Categories\")\n",
    "\tplt.savefig(f\"figures/category_histogram.pdf\")\n",
    "\n",
    "\n",
    "def q1():\n",
    "\tcat_hist()\n",
    "\n",
    "## Q2\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "\tif nltk_tag.startswith('J'):\n",
    "\t\treturn wordnet.wordnet.ADJ\n",
    "\telif nltk_tag.startswith('V'):\n",
    "\t\treturn wordnet.wordnet.VERB\n",
    "\telif nltk_tag.startswith('N'):\n",
    "\t\treturn wordnet.wordnet.NOUN\n",
    "\telif nltk_tag.startswith('R'):\n",
    "\t\treturn wordnet.wordnet.ADV\n",
    "\telse:\n",
    "\t\treturn wordnet.wordnet.NOUN\n",
    "\n",
    "def build_lemmatizing_tokenizer(tokenizer):\n",
    "\tlemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "\t# https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/\n",
    "\tdef better_tokenizer(*args,**kwargs):\n",
    "\t\ttokens = tokenizer(*args,**kwargs)\n",
    "\t\treturn [lemmatizer.lemmatize(word,pos=pos_tagger(pos)) for (word,pos) in nltk.pos_tag(tokens)]\n",
    "\treturn better_tokenizer\n",
    "\n",
    "# vectorizer_type can be 'vanilla','lemmatizing', or 'lemmatizing2'\n",
    "# (?u)\\b(\\S+)?[a-zA-Z]+(\\S+)?\\b works on regexr but has issue here\n",
    "# r\"(?u)\\b(\\d*[a-zA-Z]+\\d*)+\\b\"\n",
    "# r\"(?u)\\b(\\S*[a-zA-Z]+\\S*)+\\b\"\n",
    "def get_vectorizer():\n",
    "\tvectorizer = CountVectorizer(\n",
    "\t\tstrip_accents='unicode',\n",
    "\t\tstop_words='english',\n",
    "\t\tngram_range=(1,1), #default (1,1)\n",
    "\t\ttoken_pattern=r\"(?u)\\b(\\d*[a-zA-Z]+\\d*)+\\b\", #default r\"(?u)\\b\\w\\w+\\b\"\n",
    "\t\tmin_df=3\n",
    "\t)\n",
    "\tbetter_tokenizer = build_lemmatizing_tokenizer(vectorizer.build_tokenizer())\n",
    "\tbetter_vectorizer = CountVectorizer(\n",
    "\t\ttokenizer=better_tokenizer,\n",
    "\t\tstrip_accents='unicode',\n",
    "\t\tstop_words='english',\n",
    "\t\tngram_range=(1,1), #default (1,1)\n",
    "\t\t# token_pattern=r\"(?u)\\b[A-Za-z_][A-Za-z_]+\\b\", #tokenizer builtin\n",
    "\t\tmin_df=3\n",
    "\t)\n",
    "\treturn better_vectorizer\n",
    "\n",
    "def get_train_test(categories = ['comp.graphics', 'comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'rec.autos', 'rec.motorcycles','rec.sport.baseball', 'rec.sport.hockey'\n",
    "\t]):\n",
    "\ttwenty_train = fetch_20newsgroups(subset = 'train', categories = categories, shuffle = True, random_state = None)\n",
    "\ttwenty_test = fetch_20newsgroups(subset = 'test', categories = categories, shuffle = True, random_state = None)\n",
    "\treturn twenty_train,twenty_test\n",
    "\n",
    "def get_tf_idf(train_data,test_data):\n",
    "\tvectorizer = get_vectorizer()\n",
    "\tX_train_counts = vectorizer.fit_transform(train_data)\n",
    "\tX_test_counts = vectorizer.transform(test_data)\n",
    "\ttfidf_transformer = TfidfTransformer()\n",
    "\tX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\tX_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\treturn X_train_tfidf, X_test_tfidf\n",
    "\n",
    "def q2():\n",
    "\ttwenty_train,twenty_test = get_train_test()\n",
    "\tX_train_tfidf, X_test_tfidf = get_tf_idf(twenty_train.data,twenty_test.data)\n",
    "\tprint(f\"TF-IDF Train Matrix Shape: {X_train_tfidf.shape}\")\n",
    "\tprint(f\"TF-IDF Test Matrix Shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q3\n",
    "\n",
    "\n",
    "def ss_error(mat1,mat2):\n",
    "\treturn np.linalg.norm(mat1-mat2,ord='fro')**2\n",
    "\n",
    "# Latent Sematic Indexing\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html\n",
    "# Used to test whether LSI implementation is correct. By visual inspection it is!\n",
    "# Note scipy.sparse.linalg.svds orders signular values from least to greatest, so I flipped all matrices.\n",
    "def LSI_builtin(X,X_test):\n",
    "\tsvd = TruncatedSVD(n_components=50,n_iter=25)\n",
    "\tX_reduced= svd.fit_transform(X)\n",
    "\tX_test_reduced = svd.transform(X_test)\n",
    "\treturn X_reduced, X_test_reduced\n",
    "\n",
    "\n",
    "def get_LSI(X,X_test):\n",
    "\tU, Sigma, Vh = svds(X,k=50)\n",
    "\tU = np.flip(U,axis=1)\n",
    "\tSigma = np.flip(Sigma)\n",
    "\tVh = np.flip(Vh,axis=0)\n",
    "\tX_reduced = U.dot(np.diag(Sigma))\n",
    "\tX_test_reduced =  X_test.dot(Vh.T)\n",
    "\ttrain_error = ss_error(X, X_reduced.dot(Vh))\n",
    "\ttest_error =  ss_error(X_test, X_test_reduced.dot(Vh))\n",
    "\treturn X_reduced, X_test_reduced, train_error, test_error\n",
    "\n",
    "# Non-negative Matrix Factorization\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "def get_NMF(X,X_test):\n",
    "\tmodel = NMF(n_components=50)\n",
    "\tW = model.fit_transform(X)\n",
    "\tW_test = model.transform(X_test)\n",
    "\tH = model.components_\n",
    "\ttrain_error = ss_error(X, W.dot(H))\n",
    "\ttest_error = ss_error(X_test,W_test.dot(H))\n",
    "\treturn W, W_test, train_error,test_error\n",
    "\n",
    "\n",
    "def q3(mse=True):\n",
    "\ttwenty_train,twenty_test = get_train_test()\n",
    "\tX_train_tfidf, X_test_tfidf = get_tf_idf(twenty_train.data,twenty_test.data)\n",
    "\tX_train_LSI,X_test_LSI,LSI_train_error,LSI_test_error = get_LSI(X_train_tfidf,X_test_tfidf)\n",
    "\tX_train_NMF,X_test_NMF,NMF_train_error,NMF_test_error = get_NMF(X_train_tfidf,X_test_tfidf)\n",
    "\tprint(\"LSI:\")\n",
    "\tprint(f\"Train Error: {LSI_train_error}, Test Error: {LSI_test_error}\")\n",
    "\tprint(\"NMF:\")\n",
    "\tprint(f\"Train Error: {NMF_train_error}, Test Error: {NMF_test_error}\")\n",
    "\tif mse:\n",
    "\t\tn_train = len(twenty_train.data)\n",
    "\t\tn_test = len(twenty_test.data)\n",
    "\t\tLSI_train_MSE = LSI_train_error/n_train\n",
    "\t\tLSI_test_MSE = LSI_test_error/n_test\n",
    "\t\tNMF_train_MSE = NMF_train_error/n_train\n",
    "\t\tNMF_test_MSE = NMF_test_error/n_test\n",
    "\t\tprint(\"LSI:\")\n",
    "\t\tprint(f\"Train MSE: {LSI_train_MSE}, Test MSE: {LSI_test_MSE}\")\n",
    "\t\tprint(\"NMF:\")\n",
    "\t\tprint(f\"Train MSE: {NMF_train_MSE}, Test MSE: {NMF_test_MSE}\")\n",
    "\n",
    "\n",
    "\n",
    "## Q4\n",
    "\n",
    "def get_base_filename(filepath):\n",
    "\treturn filepath.rsplit(\".\",1)[0].rsplit(\"/\",1)[-1]\n",
    "def get_binary_dataset(cats1,cats0):\n",
    "\tdata_train,data_test = get_train_test(categories = cats1 + cats0)\n",
    "\n",
    "\tcat1_inds = [data_train.target_names.index(cat) for cat in cats1]\n",
    "\t# rec_ac_inds = [binary_train.target_names.index(cat) for cat in rec_ac_cats]\n",
    "\n",
    "\tfor dataset in data_train,data_test:\n",
    "\t\tdataset.target = [1 if target_ind in cat1_inds else 0 for target_ind in dataset.target]\n",
    "\treturn data_train,data_test\n",
    "\n",
    "def plot_roc(fprs, tprs):\n",
    "\tfig, ax = plt.subplots()\n",
    "\troc_auc = auc(fprs,tprs)\n",
    "\tax.plot(fprs, tprs, lw=2, label= f'area under curve = {roc_auc : 0.04f}')\n",
    "\tax.grid(color='0.7', linestyle='--', linewidth=1)\n",
    "\tax.set_xlim([-0.1, 1.1])\n",
    "\tax.set_ylim([0.0, 1.05])\n",
    "\tax.set_xlabel('False Positive Rate',fontsize=15)\n",
    "\tax.set_ylabel('True Positive Rate',fontsize=15)\n",
    "\tax.set_title(\"ROC Curve\")\n",
    "\tax.legend(loc=\"lower right\")\n",
    "\tfor label in ax.get_xticklabels()+ax.get_yticklabels():\n",
    "\t\tlabel.set_fontsize(15)\n",
    "\treturn fig, ax\n",
    "\n",
    "def get_error_metrics(targets,predictions,scores=None,roc_filename=None):\n",
    "\tcm = confusion_matrix(targets,predictions) #cm[i,j] = # observations in group i predicted as group j\n",
    "\ttp = cm[1,1]\n",
    "\ttn = cm[0,0]\n",
    "\tfp = cm[1,0]\n",
    "\tfn = cm[0,1]\n",
    "\tprecision = tp/(tp+fp)\n",
    "\trecall = tp/(tp+ fn)\n",
    "\taccuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "\tf1 = 2*tp/(2*tp+fp + fn)\n",
    "\tif roc_filename is not None:\n",
    "\t\tfprs,tprs, _ = roc_curve(targets,scores)\n",
    "\t\tfig, ax = plot_roc(fprs,tprs)\n",
    "\t\tax.set_title(\"ROC Curve \" + get_base_filename(roc_filename))\n",
    "\t\tplt.savefig(roc_filename)\n",
    "\treturn {'recall': recall, 'precision': precision,'accuracy': accuracy,'f1': f1, 'cm': cm, 'roc': roc_filename}\n",
    "\n",
    "\n",
    "def get_folds(X,target,nfolds=5):\n",
    "\tn = len(target)\n",
    "\tn_per = int(np.ceil(n/nfolds)) #last one may be slightly smaller\n",
    "\tinds = list(range(n))\n",
    "\trandom.shuffle(inds)\n",
    "\tfolds = [sorted(inds[i:(i + n_per)]) for i in range(0, len(inds), n_per)]\n",
    "\treturn [X[fold,:] for fold in folds], [[target[i] for i in fold] for fold in folds]\n",
    "\n",
    "def get_test_train_folds(X_folds,target_folds):\n",
    "\tnfolds = len(X_folds)\n",
    "\tfor i in range(nfolds):\n",
    "\t\tX_test, target_test = X_folds[i], target_folds[i]\n",
    "\t\tX_train = np.concatenate([X_folds[j] for j in range(nfolds) if j != i])\n",
    "\t\ttarget_train = [sample_target for j in range(nfolds) if j != i for sample_target in target_folds[j] ]\n",
    "\t\tyield X_train, X_test, target_train, target_test\n",
    "\n",
    "\n",
    "def get_linear_svc(gamma,max_iter=10000,penalty='l2',dual=False,**kwargs):\n",
    "\tclassifier = LinearSVC(C=1/gamma,max_iter=max_iter,penalty=penalty,dual=dual,**kwargs)\n",
    "\treturn classifier\n",
    "\n",
    "def get_classifier_metrics(classifier,X_train,X_test,target_train,target_test,roc_filename=None):\n",
    "\tclassifier.fit(X_train,target_train)\n",
    "\tpredictions_test = classifier.predict(X_test)\n",
    "\tscores_test = classifier.decision_function(X_test)\n",
    "\treturn get_error_metrics(target_test,predictions_test,scores_test,roc_filename=roc_filename)\n",
    "\n",
    "def get_linear_svc_metrics(gamma,X_train,X_test,target_train,target_test,roc_filename=None,max_iter=10000,penalty='l2',**kwargs):\n",
    "\tprint(\"Training linear SVC\")\n",
    "\tclassifier = get_linear_svc(gamma,max_iter=10000,penalty='l2',**kwargs)\n",
    "\treturn get_classifier_metrics(classifier,X_train,X_test,target_train,target_test,roc_filename=roc_filename)\n",
    "\n",
    "\n",
    "def q4():\n",
    "\tbinary_train,binary_test = get_binary_dataset([\"comp.graphics\", \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\", \"comp.sys.mac.hardware\"],[\"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\", \"rec.sport.hockey\"])\n",
    "\tX_train_tfidf, X_test_tfidf = get_tf_idf(binary_train.data,binary_test.data)\n",
    "\tX_train_LSI,X_test_LSI,_,_ = get_LSI(X_train_tfidf,X_test_tfidf)\n",
    "\tfor gamma in (0.0001, 1000):\n",
    "\t\terror_metrics = get_linear_svc_metrics(gamma,X_train_LSI,X_test_LSI,binary_train.target,binary_test.target,roc_filename=f\"figures/roc_LSI_SVC_gamma{gamma:.04f}.pdf\")\n",
    "\t\tprint(f\"Gamma={gamma}\")\n",
    "\t\tpprint(error_metrics)\n",
    "\tnfolds = 5\n",
    "\tX_folds, target_folds = get_folds(X_train_LSI,binary_train.target,nfolds=nfolds)\n",
    "\tgammas = [10**k for k in range(-3,4)]\n",
    "\tavg_accuracies = []\n",
    "\tfor gamma in gammas:\n",
    "\t\taccuracies = []\n",
    "\t\tfor X_train, X_test, target_train, target_test in get_test_train_folds(X_folds,target_folds):\n",
    "\t\t\terror_metrics = get_linear_svc_metrics(gamma,X_train,X_test,target_train,target_test)\n",
    "\t\t\taccuracies.append(error_metrics['accuracy'])\n",
    "\t\tavg_accuracies.append(np.mean(accuracies))\n",
    "\tgamma = gammas[np.argmax(avg_accuracies)]\n",
    "\terror_metrics = get_linear_svc_metrics(gamma,X_train_LSI,X_test_LSI,binary_train.target,binary_test.target,roc_filename=f\"figures/roc_LSI_SVC_BEST{nfolds}-fold_gamma{gamma:.04f}.pdf\")\n",
    "\tprint(f\"BEST {nfolds}-fold-validated Gamma={gamma}\")\n",
    "\tpprint(error_metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q5\n",
    "\n",
    "def get_linear_logistic(penalty,gamma,max_iter=100,solver='liblinear',**kwargs):\n",
    "\tclassifier = LogisticRegression(penalty=penalty, C=1/gamma, solver=solver,**kwargs)\n",
    "\treturn classifier\n",
    "\n",
    "def get_linear_logistic_metrics(penalty,gamma,X_train,X_test,target_train,target_test,roc_filename=None,max_iter=100,solver='liblinear'):\n",
    "\tprint(\"Training linear logistic\")\n",
    "\tclassifier = get_linear_logistic(penalty,gamma,max_iter=max_iter,solver=solver)\n",
    "\treturn get_classifier_metrics(classifier,X_train,X_test,target_train,target_test,roc_filename=roc_filename)\n",
    "\n",
    "def q5():\n",
    "\tbinary_train,binary_test = get_binary_dataset([\"comp.graphics\", \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\", \"comp.sys.mac.hardware\"],[\"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\", \"rec.sport.hockey\"])\n",
    "\tX_train_tfidf, X_test_tfidf = get_tf_idf(binary_train.data,binary_test.data)\n",
    "\tX_train_LSI,X_test_LSI,_,_ = get_LSI(X_train_tfidf,X_test_tfidf)\n",
    "\t# No regularization.\n",
    "\terror_metrics_no_reg = get_linear_logistic_metrics('none',0,X_train_LSI,X_test_LSI,binary_train.target,binary_test.target,roc_filename=\"figures/roc_LogReg_NoReg.png\")\n",
    "\tprint(\"Gamma=0\")\n",
    "\tpprint(error_metrics_no_reg)\n",
    "\n",
    "\t# Regularization. For high regularization for l1, the true positives become 0.\n",
    "\tnfolds = 5\n",
    "\tX_folds, target_folds = get_folds(X_train_LSI,binary_train.target,nfolds=nfolds)\n",
    "\tgammas = [10**k for k in range(-3,4)]\n",
    "\n",
    "\t#L1 Regularization\n",
    "\tavg_accuracies = []\n",
    "\tfor gamma in gammas:\n",
    "\t\taccuracies = []\n",
    "\t\tfor X_train, X_test, target_train, target_test in get_test_train_folds(X_folds,target_folds):\n",
    "\t\t\terror_metrics = get_linear_logistic_metrics('l1',gamma,X_train,X_test,target_train,target_test)\n",
    "\t\t\taccuracies.append(error_metrics['accuracy'])\n",
    "\t\tavg_accuracies.append(np.mean(accuracies))\n",
    "\tgamma = gammas[np.argmax(avg_accuracies)]\n",
    "\terror_metrics = get_linear_logistic_metrics('l1',gamma,X_train,X_test,target_train,target_test, roc_filename=f\"figures/roc_LogReg_l1_gamma_{gamma:.04f}.png\")\n",
    "\tprint(f\"BEST L1, {nfolds}-fold-validated Gamma={gamma}\")\n",
    "\tpprint(error_metrics)\n",
    "\n",
    "\t#L2 Regularization\n",
    "\tavg_accuracies = []\n",
    "\tfor gamma in gammas:\n",
    "\t\taccuracies = []\n",
    "\t\tfor X_train, X_test, target_train, target_test in get_test_train_folds(X_folds,target_folds):\n",
    "\t\t\terror_metrics = get_linear_logistic_metrics('l2',gamma,X_train,X_test,target_train,target_test)\n",
    "\t\t\taccuracies.append(error_metrics['accuracy'])\n",
    "\t\tavg_accuracies.append(np.mean(accuracies))\n",
    "\tgamma = gammas[np.argmax(avg_accuracies)]\n",
    "\terror_metrics = get_linear_logistic_metrics('l2',gamma,X_train,X_test,target_train,target_test, roc_filename=f\"figures/roc_LogReg_l2_gamma_{gamma:.04f}.png\")\n",
    "\tprint(f\"BEST L2, {nfolds}-fold-validated Gamma={gamma}\")\n",
    "\tpprint(error_metrics)\n",
    "\n",
    "def get_GaussianNB_metrics(X_train,X_test,target_train,target_test,roc_filename=None):\n",
    "\tclassifier = GaussianNB()\n",
    "\tclassifier.fit(X_train, target_train)\n",
    "\tpredictions_test = classifier.predict(X_test)\n",
    "\tscores_test = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\treturn get_error_metrics(target_test,predictions_test, scores_test, roc_filename=roc_filename)\n",
    "\n",
    "\n",
    "def q6():\n",
    "\tbinary_train,binary_test = get_binary_dataset([\"comp.graphics\", \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\", \"comp.sys.mac.hardware\"],[\"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\", \"rec.sport.hockey\"])\n",
    "\tX_train_tfidf, X_test_tfidf = get_tf_idf(binary_train.data,binary_test.data)\n",
    "\tX_train_LSI,X_test_LSI,_,_ = get_LSI(X_train_tfidf,X_test_tfidf)\n",
    "\n",
    "\terror_metrics = get_GaussianNB_metrics(X_train_LSI,X_test_LSI,binary_train.target,binary_test.target,roc_filename=\"figures/roc_GaussianNB.png\")\n",
    "\tprint(\"GaussianNB\")\n",
    "\tpprint(error_metrics)\n",
    "\n",
    "\n",
    "\n",
    "## Q9\n",
    "def get_glove_embeddings(dimension_of_glove=300):\n",
    "\tembeddings_dict = {}\n",
    "\twith open(f\"{get_data_home()}/glove.6B/glove.6B.{dimension_of_glove}d.txt\", 'r') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tvalues = line.split()\n",
    "\t\t\tword = values[0]\n",
    "\t\t\tvector = np.asarray(values[1:], \"float32\")\n",
    "\t\t\tembeddings_dict[word] = vector\n",
    "\treturn embeddings_dict\n",
    "\n",
    "\n",
    "def get_glove_article_matrix(X_count,vocabulary,embeddings_dict,dimension_of_glove=300):\n",
    "\tn = X_count.shape[0]\n",
    "\tX_glove = np.zeros((n,dimension_of_glove))\n",
    "\tfor i,row in enumerate(X_count):\n",
    "\t\tX_glove[i,:] = sum(row[0,ind]*embeddings_dict.get(vocabulary[ind],np.zeros(dimension_of_glove)) for ind in row.indices)/np.sum(row) # divide by number of words in article to normalize. Is there a better\n",
    "\treturn X_glove\n",
    "\n",
    "def get_classifiers_q9():\n",
    "\tgammas = [10**k for k in range(-3,4)]\n",
    "\tpenalties = ('l1','l2')\n",
    "\tlogistic_solvers = ('lbfgs', 'liblinear')\n",
    "\tclassifiers = []\n",
    "\tfor gamma in gammas:\n",
    "\t\tfor penalty in penalties:\n",
    "\t\t\tclassifiers.append(get_linear_svc(gamma,max_iter=10000,penalty=penalty))\n",
    "\t\t\tfor solver in logistic_solvers:\n",
    "\t\t\t\tif penalty == 'l2':\n",
    "\t\t\t\t\tclassifiers.append(get_linear_logistic(penalty,gamma,max_iter=1000,solver=solver))\n",
    "\treturn classifiers\n",
    "\n",
    "# LinearSVC(C=0.1, dual=False, max_iter=10000) with parameters {'C': 0.1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 10000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 0}\n",
    "def get_one_good_classifier_q9():\n",
    "\treturn LinearSVC(**{'C': 0.1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'loss': 'squared_hinge', 'max_iter': 10000, 'multi_class': 'ovr', 'penalty': 'l2', 'random_state': None, 'tol': 0.0001, 'verbose': 0})\n",
    "\n",
    "def get_cross_validated_classifier(classifiers,X_train_glove,X_test_glove,binary_train,binary_test,nfolds=5):\n",
    "\tclassifiers = get_classifiers_q9()\n",
    "\tnfolds = 5\n",
    "\tX_folds, target_folds = get_folds(X_train_glove,binary_train.target,nfolds=nfolds)\n",
    "\tavg_accuracies = []\n",
    "\tfor classifier in classifiers:\n",
    "\t\taccuracies = []\n",
    "\t\tfor X_train, X_test, target_train, target_test in get_test_train_folds(X_folds,target_folds):\n",
    "\t\t\terror_metrics = get_classifier_metrics(classifier,X_train,X_test,target_train,target_test)\n",
    "\t\t\taccuracies.append(error_metrics['accuracy'])\n",
    "\t\tavg_accuracies.append(np.mean(accuracies))\n",
    "\tclassifier = classifiers[np.argmax(avg_accuracies)]\n",
    "\treturn classifier\n",
    "\n",
    "\n",
    "def q9(dimension_of_glove=300):\n",
    "\tbinary_train,binary_test = get_binary_dataset([\"comp.graphics\", \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\", \"comp.sys.mac.hardware\"],[\"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\", \"rec.sport.hockey\"])\n",
    "\tvectorizer = CountVectorizer(strip_accents='unicode',stop_words='english',ngram_range=(1,1),token_pattern=r\"(?u)\\b(\\d*[a-zA-Z]+\\d*)+\\b\",min_df=3)\n",
    "\tX_train_counts = vectorizer.fit_transform(binary_train.data)\n",
    "\tX_test_counts = vectorizer.transform(binary_test.data)\n",
    "\tvocabulary = vectorizer.get_feature_names()\n",
    "\tembeddings_dict = get_glove_embeddings(dimension_of_glove=dimension_of_glove)\n",
    "\tX_train_glove = get_glove_article_matrix(X_train_counts,vocabulary,embeddings_dict,dimension_of_glove)\n",
    "\tX_test_glove = get_glove_article_matrix(X_test_counts,vocabulary,embeddings_dict,dimension_of_glove)\n",
    "\tclassifiers = get_classifiers_q9()\n",
    "\tclassifier = get_cross_validated_classifier(classifiers,X_train_glove,X_test_glove,binary_train,binary_test,nfolds=5)\n",
    "\tclassifier_name = str(classifier)\n",
    "\terror_metrics =  get_classifier_metrics(classifier,X_train_glove,X_test_glove,binary_train.target,binary_test.target,roc_filename=f\"figures/roc_GloVe_cross-validated_{classifier_name}.pdf\")\n",
    "\tprint(f\"{classifier} with parameters {classifier.get_params()}\")\n",
    "\tpprint(error_metrics)\n",
    "\n",
    "\n",
    "## Q10\n",
    "def q10():\n",
    "\tbinary_train,binary_test = get_binary_dataset([\"comp.graphics\", \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\", \"comp.sys.mac.hardware\"],[\"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\", \"rec.sport.hockey\"])\n",
    "\tvectorizer = CountVectorizer(strip_accents='unicode',stop_words='english',ngram_range=(1,1),token_pattern=r\"(?u)\\b(\\d*[a-zA-Z]+\\d*)+\\b\",min_df=3)\n",
    "\tX_train_counts = vectorizer.fit_transform(binary_train.data)\n",
    "\tX_test_counts = vectorizer.transform(binary_test.data)\n",
    "\tvocabulary = vectorizer.get_feature_names()\n",
    "\tdimensions = (50,100,200,300)\n",
    "\taccuracies = {}\n",
    "\tfor dimension_of_glove in dimensions:\n",
    "\t\tembeddings_dict = get_glove_embeddings(dimension_of_glove=dimension_of_glove)\n",
    "\t\tX_train_glove = get_glove_article_matrix(X_train_counts,vocabulary,embeddings_dict,dimension_of_glove)\n",
    "\t\tX_test_glove = get_glove_article_matrix(X_test_counts,vocabulary,embeddings_dict,dimension_of_glove)\n",
    "\t\tclassifier = get_one_good_classifier_q9()\n",
    "\t\terror_metrics =  get_classifier_metrics(classifier,X_train_glove,X_test_glove,binary_train.target,binary_test.target)\n",
    "\t\taccuracies[dimension_of_glove] = error_metrics['accuracy']\n",
    "\tclassifier_name = str(classifier)\n",
    "\t# plt.plot(tuple(accuracies.keys()), tuple(accuracies.values()))\n",
    "\tfig, ax = plt.subplots()\n",
    "\tax.plot(tuple(accuracies.keys()), tuple(accuracies.values()))\n",
    "\tax.set_xlabel('GloVe Embedding Dimension',fontsize=15)\n",
    "\tax.set_ylabel('Accuracy',fontsize=15)\n",
    "\tax.set_title(f\"GloVe Accuracy vs. Embedding Dimension\\nwith {classifier_name}\")\n",
    "\tplt.savefig(f\"figures/GloVe_accuracy_vs_dimension_{classifier_name}.pdf\")\n",
    "\n",
    "def get_random_matrix_and_labels(nsamples,dim):\n",
    "\treturn np.random.rand(nsamples,dim), np.full(nsamples,-1).tolist()\n",
    "\n",
    "def q11():\n",
    "\tbinary_train,binary_test = get_binary_dataset([\"comp.graphics\", \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\", \"comp.sys.mac.hardware\"],[\"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\", \"rec.sport.hockey\"])\n",
    "\ttarget_train = binary_train.target\n",
    "\ttarget_names = [ \"Recreational Activity\",\"Computer Technology\"]\n",
    "\t# target_test = binary_test.target\n",
    "\tvectorizer = CountVectorizer(strip_accents='unicode',stop_words='english',ngram_range=(1,1),token_pattern=r\"(?u)\\b(\\d*[a-zA-Z]+\\d*)+\\b\",min_df=3)\n",
    "\tX_train_counts = vectorizer.fit_transform(binary_train.data)\n",
    "\t# X_test_counts = vectorizer.transform(binary_test.data)\n",
    "\tvocabulary = vectorizer.get_feature_names()\n",
    "\tdimension_of_glove = 300\n",
    "\tembeddings_dict = get_glove_embeddings(dimension_of_glove=dimension_of_glove)\n",
    "\tX_train_glove = get_glove_article_matrix(X_train_counts,vocabulary,embeddings_dict,dimension_of_glove)\n",
    "\t# X_test_glove = get_glove_article_matrix(X_test_counts,vocabulary,embeddings_dict,dimension_of_glove)\n",
    "\tn_articles = X_train_glove.shape[0]\n",
    "\tX_random, target_random = get_random_matrix_and_labels(nsamples=n_articles,dim=dimension_of_glove)\n",
    "\treducer = umap.UMAP()\n",
    "\tX_random_umap = reducer.fit_transform(X_random)\n",
    "\tX_random_umap_normalized = StandardScaler().fit_transform(X_random_umap)\n",
    "\treducer = umap.UMAP()\n",
    "\tX_umap = reducer.fit_transform(X_train_glove)\n",
    "\tX_umap_normalized = StandardScaler().fit_transform(X_umap)\n",
    "\tall_colors = ['red','green','blue','orange']\n",
    "\tcmap = {target : all_colors.pop() for target in set(target_train + target_random)}\n",
    "\tfig, ax = plt.subplots()\n",
    "\tax.scatter(\n",
    "\t\tX_random_umap_normalized[:,0],\n",
    "\t\tX_random_umap_normalized[:,1],\n",
    "\t\tc=[cmap[-1] for i in target_random],\n",
    "\t\ts=4,\n",
    "\t\talpha = 0.5,\n",
    "\t\tlabel=\"Random Vectors\"\n",
    "\t)\n",
    "\tfor target_label in set(target_train):\n",
    "\t\tinds = [i for i,val in enumerate(target_train) if val == target_label]\n",
    "\t\tax.scatter(\n",
    "\t\t\tX_umap_normalized[inds,0],\n",
    "\t\t\tX_umap_normalized[inds,1],\n",
    "\t\t\tc=[cmap[target_train[i]] for i in inds],\n",
    "\t\t\ts=4,\n",
    "\t\t\talpha = 0.5,\n",
    "\t\t\tlabel=f\"Class {target_label} ({target_names[target_label]})\",\n",
    "\t\t)\n",
    "\tax.legend()\n",
    "\tplt.title('UMAP Projection of the GloVe Article Embeddings')\n",
    "\tplt.savefig(f\"figures/umap_glove_vs_random.pdf\")\n",
    "\t# plt.show(block=False)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\tq9()\n",
    "\tpass\n",
    "\n",
    "\n",
    "# q1()\n",
    "# q2()\n",
    "# q3()\n",
    "# q4()\n",
    "# q5()\n",
    "# q6()\n",
    "# pass\n",
    "# q9()\n",
    "# q11()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "everyday-hayes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 436 kB/s eta 0:00:01     |███████▌                        | 337 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/boyaouyang/opt/anaconda3/envs/python39/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 461 kB/s eta 0:00:01     |████████                        | 20 kB 246 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2020.11.13-cp39-cp39-macosx_10_9_x86_64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 403 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434675 sha256=95c71487557d3dc0958a850b813f11a87b61232ebce985adb98721d6c9770f90\n",
      "  Stored in directory: /Users/boyaouyang/Library/Caches/pip/wheels/13/ae/bb/5e2a232ebaa1d2f38dd5f587e9fc4cf6ccb12758d14dac14d8\n",
      "Successfully built nltk\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.5 regex-2020.11.13 tqdm-4.59.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ethical-computer",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8ae672593980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcaching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/python39/lib/python3.9/site-packages/hdbscan/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhdbscan_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHDBSCAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrobust_single_linkage_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobustSingleLinkage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrobust_single_linkage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvalidity\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidity_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m from .prediction import (approximate_predict,\n\u001b[1;32m      5\u001b[0m                          \u001b[0mmembership_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/python39/lib/python3.9/site-packages/hdbscan/hdbscan_.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from ._hdbscan_linkage import (single_linkage,\n\u001b[0m\u001b[1;32m     22\u001b[0m                                \u001b[0mmst_linkage_core\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                \u001b[0mmst_linkage_core_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mhdbscan/_hdbscan_linkage.pyx\u001b[0m in \u001b[0;36minit hdbscan._hdbscan_linkage\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import datasets, cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "import umap\n",
    "from caching import cached\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "banned-harris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdbscan in /Users/boyaouyang/opt/anaconda3/envs/python39/lib/python3.9/site-packages (0.8.27)\n",
      "Requirement already satisfied: joblib>=1.0 in /Users/boyaouyang/opt/anaconda3/envs/python39/lib/python3.9/site-packages (from hdbscan) (1.0.1)\n",
      "Requirement already satisfied: six in /Users/boyaouyang/opt/anaconda3/envs/python39/lib/python3.9/site-packages (from hdbscan) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /Users/boyaouyang/opt/anaconda3/envs/python39/lib/python3.9/site-packages (from hdbscan) (1.19.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/boyaouyang/opt/anaconda3/envs/python39/lib/python3.9/site-packages (from hdbscan) (1.6.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /Users/boyaouyang/opt/anaconda3/envs/python39/lib/python3.9/site-packages (from hdbscan) (0.24.1)\n",
      "Requirement already satisfied: cython>=0.27 in /Users/boyaouyang/opt/anaconda3/envs/python39/lib/python3.9/site-packages (from hdbscan) (0.29.22)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/boyaouyang/opt/anaconda3/envs/python39/lib/python3.9/site-packages (from scikit-learn>=0.20->hdbscan) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "understanding-aquatic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyaouyang/opt/anaconda3/envs/python39/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make', 'u'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Train Matrix Shape: (4732, 15805)\n",
      "TF-IDF Test Matrix Shape: (3150, 15805)\n"
     ]
    }
   ],
   "source": [
    "q2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "boring-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(categories = cats1 + cats0,shuffle=False,random_state=None,remove=('headers','footers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "medium-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats1=[\"comp.graphics\", \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\", \"comp.sys.mac.hardware\"]\n",
    "cats0=[\"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\", \"rec.sport.hockey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "numeric-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1_inds = [data.target_names.index(cat) for cat in cats1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "royal-booth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat1_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lucky-florida",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import datasets, cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "import umap\n",
    "from caching import cached\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_all_data(categories = None):\n",
    "\treturn fetch_20newsgroups(categories=categories,shuffle=False,random_state=None,remove=('headers','footers'))\n",
    "\n",
    "\n",
    "\n",
    "def get_binary_dataset(cats1=[\"comp.graphics\", \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\", \"comp.sys.mac.hardware\"],cats0=[\"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\", \"rec.sport.hockey\"]):\n",
    "\tdata = fetch_20newsgroups(categories = cats1 + cats0,shuffle=False,random_state=None,remove=('headers','footers'))\n",
    "\tcat1_inds = [data.target_names.index(cat) for cat in cats1]\n",
    "\tdata.target = [1 if target_ind in cat1_inds else 0 for target_ind in data.target]\n",
    "\treturn data\n",
    "\n",
    "\n",
    "def get_vectorizer(basic=False):\n",
    "\treturn CountVectorizer(\n",
    "\t\tstrip_accents='unicode',\n",
    "\t\tstop_words='english',\n",
    "\t\tngram_range=(1,1), #default (1,1)\n",
    "\t\ttoken_pattern=r\"(?u)\\b(\\d*[a-zA-Z]+\\d*)+\\b\", #default r\"(?u)\\b\\w\\w+\\b\"\n",
    "\t\tmin_df=3\n",
    "\t)\n",
    "\n",
    "@cached\n",
    "def get_tf_idf(train_data):\n",
    "\tvectorizer = get_vectorizer(basic=True) #no lemmatization\n",
    "\tX_train_counts = vectorizer.fit_transform(train_data)\n",
    "\ttfidf_transformer = TfidfTransformer()\n",
    "\tX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\treturn X_train_tfidf\n",
    "\n",
    "\n",
    "def get_contingency(ground_truth_target,predictions):\n",
    "\tA = np.zeros((len(set(ground_truth_target)),len(set(predictions))))\n",
    "\tfor (i,j) in zip(ground_truth_target,predictions):\n",
    "\t\tA[i,j] += 1\n",
    "\treturn A\n",
    "\n",
    "@cached\n",
    "def get_KMeans(X,n_clusters=8):\n",
    "\tkmeans = KMeans(\n",
    "\t\tn_clusters=n_clusters,\n",
    "\t\trandom_state=0,\n",
    "\t\tmax_iter = 2000,\n",
    "\t\tn_init = 50\n",
    "\t)\n",
    "\tpredictions = kmeans.fit_predict(X)\n",
    "\treturn predictions,kmeans\n",
    "\n",
    "\n",
    "def get_cluster_metrics(labels,predictions):\n",
    "\tmetric_fns = (\n",
    "\t\tmetrics.homogeneity_score,\n",
    "\t\tmetrics.completeness_score,\n",
    "\t\tmetrics.v_measure_score,\n",
    "\t\tmetrics.adjusted_rand_score,\n",
    "\t\tmetrics.mutual_info_score,\n",
    "\t\tmetrics.balanced_accuracy_score\n",
    "\t)\n",
    "\treturn { fn.__name__ : fn(labels,predictions) for fn in metric_fns}\n",
    "\n",
    "\n",
    "@cached\n",
    "def LSI_builtin(X,n_components):\n",
    "\tsvd = TruncatedSVD(n_components=n_components)\n",
    "\tX_reduced= svd.fit_transform(X)\n",
    "\treturn X_reduced,svd\n",
    "\n",
    "def get_LSI_matrix(*args,**kwargs):\n",
    "\tX_reduced,svd = LSI_builtin(*args,**kwargs)\n",
    "\treturn X_reduced\n",
    "\n",
    "def ss_error(mat1,mat2):\n",
    "\treturn np.linalg.norm(mat1-mat2,ord='fro')**2\n",
    "\n",
    "@cached\n",
    "def get_NMF(X,n_components):\n",
    "\tmodel = NMF(n_components=n_components)\n",
    "\tW = model.fit_transform(X)\n",
    "\treturn W\n",
    "\n",
    "\n",
    "def close_factors(n):\n",
    "\treturn min([(n//i,i,abs((n//i) - i)) for i in range(1, int(n**0.5)+1) if n % i == 0],key=lambda tup: tup[2])[0:2]\n",
    "\n",
    "def get_KL_NMF(X,n_components,beta_loss = 'kullback-leibler', solver = 'mu'):\n",
    "\tmodel = NMF(n_components=n_components,beta_loss = beta_loss, solver=solver)\n",
    "\tW = model.fit_transform(X)\n",
    "\treturn W\n",
    "\n",
    "def UMAP_builtin(X,n_components,metric):\n",
    "\treducer =umap.UMAP(metric=metric,n_components=n_components)\n",
    "\tX_reduced= reducer.fit_transform(X)\n",
    "\treturn X_reduced,reducer\n",
    "\n",
    "def get_UMAP_matrix(*args,**kwargs):\n",
    "\tX_reduced,reducer = UMAP_builtin(*args,**kwargs)\n",
    "\treturn X_reduced\n",
    "\n",
    "def AgglomerativeClustering_builtin(X,n_clusters,linkage):\n",
    "\treducer = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkage)\n",
    "\tX_reduced= reducer.fit_predict(X)\n",
    "\treturn X_reduced,reducer\n",
    "\n",
    "def get_AgglomerativeClustering(*args,**kwargs):\n",
    "\tX_reduced,reducer = AgglomerativeClustering_builtin(*args,**kwargs)\n",
    "\treturn X_reduced\n",
    "\n",
    "def HDBSCAN(X,min_samples,alpha,metric):\n",
    "\tclusterer = hdbscan.HDBSCAN(min_cluster_size=100,min_samples=min_samples,alpha=alpha,metric=metric)\n",
    "\tclusterer.fit(X)\n",
    "\t\n",
    "\treturn clusterer.labels_, clusterer\n",
    "\n",
    "def DBSCAN_builtin(X,eps,metric):\n",
    "\tclusterer = DBSCAN(eps=eps,min_samples=100,metric=metric)\n",
    "\tclusterer.fit(X)\n",
    "\treturn clusterer.labels_, clusterer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
